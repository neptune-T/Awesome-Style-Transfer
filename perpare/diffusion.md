扩散模型的出现重塑了生成建模的格局，为解决复杂数据分布提供了一种创新方法。受扩散的物理过程启发，这些模型在正向过程中将数据转换为噪声，然后逆向转换以生成连贯而真实的输出。这种迭代去噪过程可以实现无与伦比的高维数据重建精度，使扩散模型成为现代生成框架的基石。

与 GAN 等对抗性训练方法不同，扩散模型依赖于基于可能性的优化框架，从而避免了对抗性设置中经常出现的不稳定性。强大的训练范式，加上其生成多样化和高保真输出的能力，使扩散模型成为从图像合成到文本到图像生成和风格转换等领域的多功能工具。

![[Pasted image 20241124150124.png]]

在正向过程中，目标是通过在 \( T \) 个离散时间步中添加高斯噪声来迭代破坏数据样本 \( x_0 \)。给定上一步 \( t-1 \)，步骤 \( t \) 处损坏数据的条件分布定义为：

\[
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I),
\]

其中 \( \beta_t \) 表示控制每一步添加的噪声量的方差计划。在连续的步骤中，数据逐渐转变为纯高斯噪声分布​​。这种累积效应是通过将 \( x_t \) 直接关联到初始数据 \( x_0 \) 来捕获的：

\[
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I),
\]

其中 \( \bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s) \) 为噪声计划的累积乘积。此属性确保了数学上可处理的正向过程并有助于高效采样。

旨在撤消正向扩散的反向过程学习逐步对 \( x_t \) 进行去噪以重建 \( x_0 \)。与正向过程不同，反向过程使用神经网络 \( p_\theta(x_{t-1}|x_t) \) 进行参数化，因为真正的反向分布 \( q(x_{t-1}|x_t) \) 是难以处理的。此反向过程可以表示为：

\[
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)),
\]

![[Pasted image 20241124150401.png]]

与朗之万之间的联系

朗之万动力学是物理学中的一个概念，用于对分子系统进行统计建模。结合随机梯度下降，_随机梯度朗之万动力学_（[Welling & Teh 2011](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)）可以从概率密度中产生样本页（十）仅使用渐变∇十
日志⁡页（十）在马尔可夫更新链中：
![[Pasted image 20241124150437.png]]

与标准 SGD 相比，随机梯度朗之万动力学将高斯噪声注入参数更新中，以避免陷入局部最小值。![[Pasted image 20241124150518.png]]



反向过程旨在消除正向扩散，它学习逐步消除 \( x_t \) 的噪声以重建 \( x_0 \)。与正向过程不同，反向过程使用神经网络 \( p_\theta(x_{t-1}|x_t) \) 进行参数化，因为真正的反向分布 \( q(x_{t-1}|x_t) \) 是难以处理的。这个逆过程可以表示为：

\[
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)),
\]

其中 \( \mu_\theta \) 和 \( \Sigma_\theta \) 是学习到的参数，表示逆高斯分布的均值和方差。通过迭代应用这些学习到的变换，模型从纯噪声开始生成真实的数据。

训练扩散模型涉及最小化实际反向转换 \( q(x_{t-1}|x_t, x_0) \) 与其参数化近似值 \( p_\theta(x_{t-1}|x_t) \) 之间的差异。这是通过变分下限 (VLB) 目标实现的：

\[
\begin{aligned}
L_{\text{VLB}} = \mathbb{E}_q \bigg[
\sum_{t=1}^T & D_{\text{KL}}(q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t)) \\
& - \log p_\theta(x_0|x_1)
\bigg].
\end{aligned}
\]

此目标确保逆向过程能够学会从噪声中准确地重建数据分布。通过利用这种逐步细化，扩散模型不仅可以在捕捉高维数据分布的复杂性的同时实现稳定高效的训练，还可以避免 GAN 中常见的对抗性训练陷阱，例如模式崩溃。







自首次推出以来，扩散模型的发展取得了重大进展，特别是随着去噪扩散概率模型 (DDPM) 的提出，它增强了马尔可夫链框架的稳定性。随后，去噪扩散隐式模型 (DDIM) 引入了一种新颖的优化方法，该方法将变分下限 (VLB) 目标与 DDPM 的简化目标函数相结合。在这种混合目标下，与直接优化似然相比，所提出的模型实现了更好的对数似然。此外，据观察，后者目标在训练过程中引入了更多的梯度噪声，可能会影响收敛。

DDIM 带来的最具突破性的进步之一是它能够减少推理步骤。虽然原始 DDPM 需要多达 1000 个步骤进行反向采样，但 DDIM 优化了反向扩散过程中的噪声调度，修改了 $x_t$ 的递归公式，并以明显更少的步骤（例如 100 步）实现了可比的结果。这一成果具有革命性，此后被大量扩散模型研究采用，特别是在涉及计算密集型任务的场景中。

后续进展包括“扩散模型在图像合成方面击败 GAN”的提议，该提议对 UNet 架构进行了多项改进，并加入了分类器指导。这种方法利用预先训练的分类器来指导生成过程，提高了样本质量和可控性。进一步的创新来自无分类器扩散指导，它引入了一种新的分数估计函数，作为条件和无条件分数函数的线性组合。这种方法平衡了两个分数函数的贡献，实现了更通用和更强大的生成。

在此基础上，Palette：图像到图像扩散模型从先前的工作中汲取灵感，实现了图像着色、修复、裁剪恢复和超分辨率等任务。Palette 的意义在于它展示了扩散模型在图像到图像转换任务中的潜力，突出了其灵活性和有效性。这一突破揭示了扩散模型在图像翻译任务中的更广阔潜力，激发了该领域的研究热潮，尤其是从 CVPR 2021 开始，相关出版物激增。

潜在扩散模型 (LDM) 的引入标志着扩散模型演变的另一个里程碑，最终催生了广受欢迎的稳定扩散。LDM 引入了两项关键创新：首先，使用编码器-解码器结构将扩散过程扩展到潜在空间（潜在域）。这种方法回归了生成建模的基本原理，同时利用潜在空间 ($z$) 显着降低计算成本，这是 VAE 框架中长期使用的想法。其次，LDM 采用了交叉注意机制，这种结构最初是在 2020 年手写扩散论文中提出的，但当时基本上被忽视了。有了 LDM，交叉注意成为多模态生成的基石和现代条件扩散模型的标准。

进一步的创新包括带有交叉注意力控制的即时图像编辑，该方法可视化了 QKV 矩阵并替换了注意力图以实现更精细的控制。与 LDM 相比，该方法对生成过程提供了更精确和更引人注目的控制。最后，带有 Transformers 的可扩展扩散模型成功地用增强的基于 Transformer 的框架取代了 UNet 架构，在高分辨率图像合成中取得了卓越的效果。这是一个重大的飞跃，展示了 Transformers 在扩散模型中的适应性和潜力。