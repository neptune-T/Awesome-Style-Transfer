\section{The Rise of Diffusion Models}

The advent of diffusion models has reshaped the landscape of generative modeling, providing an innovative approach to addressing complex data distributions. Inspired by the physical process of diffusion in nonequilibrium thermodynamics, these models define a Markov chain that transforms data into noise in the forward process and then back again in the backward process to generate coherent and realistic outputs. This iterative denoising process can achieve unparalleled reconstruction accuracy for high-dimensional data, making diffusion models a cornerstone of modern generative frameworks.

Unlike GANs, which rely on adversarial training methods, diffusion models are based on a likelihood-based optimization framework, avoiding the instabilities commonly encountered in adversarial settings. Compared to VAEs, diffusion models achieve greater diversity and higher fidelity in their outputs. As illustrated in the diagram below\ref{fig:diffusion-label}, the architecture differences among these models highlight how the robust training paradigm of diffusion models has made them versatile tools for a wide range of applications, including image synthesis, text-to-image generation, and style transformation.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{TPAMI_Survey_Template/figures/diffusion_structure.png}
    \caption{Overview of different types of generative models}
    \label{fig:diffusion-label}
\end{figure}

To understand diffusion-based generative models, it is essential to examine the forward process, which forms the foundation of these approaches \cite{sohl2015deep}\cite{song2019generative}\cite{ho2020denoising}. The goal of the forward process is to iteratively corrupt a data sample \(x_0\) by adding Gaussian noise across \(T\) discrete timesteps. At each step \(t\), the conditional distribution of the corrupted data, given the previous step \(t-1\), is defined as:

\[
q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I),
\]

where \( \beta_t \) represents a variance schedule that controls the amount of noise added at each step. Over successive steps, this process gradually transforms the data into pure Gaussian noise. The cumulative effect can be expressed by directly relating \(x_t\) to the initial data \(x_0\):

\[
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I),
\]

with \( \bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s) \), the cumulative product of the noise schedules. This formulation ensures a mathematically tractable forward process and facilitates efficient sampling.

The reverse process in diffusion models is designed to reconstruct data by progressively removing noise from \( x_t \) to recover \( x_0 \). Unlike the forward process, the reverse transition \( q(x_{t-1}|x_t) \) is intractable, necessitating parameterization via a neural network \( p_\theta(x_{t-1}|x_t) \). The reverse process is expressed as:
\[
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)),
\]
where \( \mu_\theta \) and \( \Sigma_\theta \) are learned parameters representing the mean and variance of the reverse Gaussian distribution. By iteratively applying these transformations, the model generates realistic data starting from pure noise.

Training a diffusion model involves minimizing the discrepancy between the true reverse transitions \( q(x_{t-1}|x_t, x_0) \) and their parameterized approximations \( p_\theta(x_{t-1}|x_t) \). This is achieved through a variational lower bound (VLB) objective:
\[
\begin{aligned}
L_{\text{VLB}} = \mathbb{E}_q \bigg[ 
    \sum_{t=1}^T & D_{\text{KL}}(q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t)) \\
    & - \log p_\theta(x_0|x_1) 
\bigg].
\end{aligned}
\]
This objective ensures that the reverse process learns to reconstruct data distributions accurately from noise, leveraging stepwise refinements.

Interestingly, this iterative denoising process can be linked to Langevin dynamics, a concept rooted in statistical physics that models molecular systems. Stochastic Gradient Langevin Dynamics (SGLD) \cite{welling2011bayesian} extends this idea to sampling from probability densities by combining gradient updates with Gaussian noise injection:
\[
x_t = x_{t-1} + \frac{\delta}{2} \nabla_x \log p(x_{t-1}) + \sqrt{\delta}\epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, I),
\]
where the noise term avoids local minima by ensuring exploration in the parameter space.

By introducing Gaussian noise into updates, as shown above, SGLD mirrors the reverse diffusion process in its ability to refine samples iteratively. This synergy between Langevin dynamics and diffusion models further underscores the theoretical robustness and flexibility of the framework.

The figure \ref{fig:diffusion-label} below illustrates both the forward diffusion trajectory \( q(x_0:T) \) and the reverse trajectory \( p_\theta(x_0:T) \), alongside the drifting term \( \mu_\theta(x_t, t) - x_t \). These visualizations emphasize the gradual corruption and reconstruction of data during the forward and reverse processes, highlighting the efficacy of diffusion models in capturing complex data distributions.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{TPAMI_Survey_Template/figures/diffusion_Modeling.png}
    \caption{Visualization of the forward diffusion trajectory \( q(x_0:T) \), reverse trajectory \( p_\theta(x_0:T) \), and the drifting term \( \mu_\theta(x_t, t) - x_t \) over time.}
    \label{fig:diffusion-label}
\end{figure}


This objective ensures the reverse process learns to accurately reconstruct the data distribution from noise. By leveraging this stepwise refinement, diffusion models not only achieve stable and efficient training while capturing the complexity of high-dimensional data distributions, but also avoids adversarial training pitfalls, such as mode collapse, commonly encountered in GANs. 

\subsection{Improvement and application of diffusion model}


Considering the diversity and rapid expansion of diffusion model variants, the table \ref{tab:diffusion_variants} below provides a detailed summary of their applications and contributions across different domains. It categorizes these advancements into primary and secondary tasks, offering a comprehensive understanding of how diffusion models have been tailored to address domain-specific challenges.


\begin{table*}[h!]
\centering
\renewcommand{\arraystretch}{1.5} 
\setlength{\tabcolsep}{4pt} 


\begin{tabular}{p{4.5cm}<{\centering}|p{10cm}<{\raggedright}|p{2.5cm}<{\centering}}

\hline
Primart  & Key Contributions  & Article \\ \hline
     & \textbf{Efficiency Improvement:} Solve the problems of slow sampling speed and high computational cost.  & 
     \cite{nichol2021improved}, 
     \cite{song2020denoising},
     \cite{salimans2022progressive},
     \cite{zhou2024simple},
     \cite{ma2024efficient},
     \cite{lee2024beta},
     \cite{cheng2023sampler},
     \cite{pandey2024towards},
     \cite{ulhaq2022efficient},
     \cite{dhariwal2021diffusion} \\ \cline{2-3} 
\textbf{High-Level Objectives} & \textbf{Generation Quality and Control:} Improve the realism and diversity of generated images, and enhance control over the generation process.& 
     \cite{ho2020denoising},
     \cite{dhariwal2021diffusion},
     \cite{rombach2022high},
     \cite{hong2023improving},
     \cite{zhao2023arkittrack},
     \cite{karras2024guiding},
     \cite{kang2024observation}  \\ \cline{2-3} 
     &\textbf{Practicality and Robustness:} Enhance the model's ability to cope with complexity and uncertainty in real-world scenarios.    &
     \cite{nichol2021improved},
     \cite{zhang2023diffsmooth},
     \cite{xiao2023safediffuser},
     \cite{dao2023robust}
     \\ \hline
     & \textbf{Conditional Generation:} Conditional information such as text, category labels, and semantic graphs are used to guide the diffusion model generation process, achieving flexible and high-quality multimodal generation.   & 
     \cite{ho2020denoising},
     \cite{dhariwal2021diffusion},
     \cite{rombach2022high},
     \cite{ho2022classifier},
     \cite{nichol2021glide},
     \cite{saharia2022photorealistic}
     \\ \cline{2-3} 
     & \textbf{Style Transfer and Feature Control:} Through the latent space optimization and conditional constraints of the diffusion model, diversified image style transfer is completed and fine control of generated features is achieved.& 
     \cite{chung2024style},
     \cite{yang2023zero},
     \cite{zhang2023inversion}, \cite{Zhang_Zhang_Xing_Li_Zhao_Sun_Lan_Luan_Huang_Lin_2024},
     \cite{he2024freestyle},
     \cite{li2024diffstyler}
     \\ \cline{2-3} 
\textbf{Key Research Directions}  & \textbf{Sampling Acceleration:} Significantly reduce the number of steps to generate the diffusion model, improve generation efficiency while maintaining high-quality output.   & 
    \cite{song2020denoising}, 
    \cite{nichol2021improved},
    \cite{lu2022dpm},
    \cite{salimans2022progressive},
    \cite{lyu2022accelerating},
    \cite{zheng2023fast},
    \cite{tang2024accelerating},
    \cite{xia2023towards}
    \\ \cline{2-3} 
     & \textbf{Robustness and Generalization:} Improve the performance of diffusion models in noisy environments, incomplete data, and cross-domain generation tasks, and enhance their practicality and generalization capabilities. & 
     \cite{li2023generalization},
     \cite{xiao2022densepure},
     \cite{chen2024your},
     \cite{tsai2024gda},
     \cite{chen2023robust},
     \cite{zhang2023diffsmooth}
     \\ \cline{2-3} 
     & \textbf{Model Optimization:} By improving technologies such as noise scheduling, knowledge distillation, and latent space modeling, the computational overhead can be reduced and the training stability and generation quality can be improved.  & \cite{nichol2021improved}, \cite{rombach2022high}, \cite{salimans2022progressive},
     \cite{li2023autodiffusion},
     \cite{guo2024gradient},
     \cite{krishnamoorthy2023diffusion},
     \cite{kong2024diffusion},
     \cite{li2024diffusion}
     \\ \hline
\end{tabular}
\caption{Applications and Variants of Diffusion Models Across Domains}
\label{tab:diffusion_variants}
\end{table*}


The development of diffusion models has seen significant progress since their initial introduction, particularly with the proposal of Denoising Diffusion Probabilistic Models (DDPM)\cite{ho2020denoising}, which enhanced the stability of the Markov chain framework. Following this, Denoising Diffusion Implicit Models (DDIM)\cite{song2020denoising} introduced a novel optimization approach that combined the Variational Lower Bound (VLB) objective with a simplified target function from DDPM. Under this hybrid objective, the proposed model achieved superior log-likelihood compared to directly optimizing the likelihood. Moreover, it was observed that the latter target introduced more gradient noise during training, potentially impacting convergence.

One of the most groundbreaking advancements brought by DDIM was its ability to reduce the inference steps. While the original DDPM required up to 1000 steps for reverse sampling, DDIM optimized the noise scheduling in the reverse diffusion process, modifying the recursive formula for $x_t$ and achieving comparable results with significantly fewer steps (e.g., 100 steps). This achievement was revolutionary and has since been adopted by a large proportion of diffusion model research, particularly in scenarios involving computationally intensive tasks.

Subsequent advancements include the proposal of Diffusion Models Beat GANs on Image Synthesis\cite{dhariwal2021diffusion}, which introduced several refinements to the UNet architecture and incorporated classifier guidance. This approach leveraged a pre-trained classifier to guide the generation process, improving both sample quality and controllability. Further innovation came with Classifier-Free Diffusion Guidance, which introduced a new score estimation function as a linear combination of conditional and unconditional score functions. This method balanced the contributions of both score functions, enabling more versatile and robust generation.

Building on this foundation, Palette: Image-to-Image Diffusion Models\cite{ho2021palette} drew inspiration from prior work, enabling tasks such as image colorization, inpainting, cropping recovery, and super-resolution. The significance of Palette lies in its demonstration of the potential of diffusion models in image-to-image translation tasks, highlighting their flexibility and effectiveness. This breakthrough revealed the broader potential of diffusion models in image translation tasks, inspiring a surge of research in the field, particularly starting with CVPR 2021, which saw an explosion of related publications.

The introduction of Latent Diffusion Models (LDM) \cite{rombach2022latent} marked another milestone in the evolution of diffusion models, ultimately giving rise to the widely popular Stable Diffusion. LDM introduced two key innovations: first, the use of an encoder-decoder structure to scale the diffusion process into the latent space (latent domain). This approach returned to the fundamental principles of generative modeling while leveraging the latent space ($z$) to reduce computational costs significantly, an idea long-utilized in VAE frameworks. Second, LDM employed cross-attention mechanisms, a structure initially proposed in the 2020 Handwriting Diffusion paper but largely overlooked at the time. With LDM, cross-attention became a cornerstone for multi-modal generation and a standard in modern conditional diffusion models.

Further innovations included Prompt-to-Prompt Image Editing with Cross Attention Control\cite{jin2022prompt}, which visualized the QKV matrices and replaced attention maps to achieve finer control. Compared to LDM, this method provided more precise and compelling control over the generation process. Finally, Scalable Diffusion Models with Transformers\cite{salimans2022scalable} successfully replaced the UNet architecture with an enhanced Transformer-based framework, achieving superior results in high-resolution image synthesis. This represents a significant leap forward, demonstrating the adaptability and potential of Transformers in diffusion models.

\subsection{Diffusion style transfer}

Diffusion models, as generative models based on Markov chains, were originally designed for denoising tasks, where the goal is to progressively reverse the noise destruction in an image. The key characteristic of diffusion models is that each state in the generation process depends only on the previous state, making them more suitable for generative tasks, such as image synthesis, rather than traditional denoising. Despite this, diffusion models have been effectively adapted for style transfer tasks, where they progressively transfer style features from one image to another, rather than merely performing denoising operations. The core challenge in style transfer lies in how to preserve the content of the target image while accurately transferring the style features, and diffusion models have shown significant potential in this context.

To achieve style transfer within the diffusion model framework, several techniques have been introduced to guide the model in transferring style features from one image to another. Among these, CLIP (Contrastive Language-Image Pretraining)\cite{radford2021learningtransferablevisualmodels} plays a crucial role. CLIP maps both images and text to a shared feature space, enabling effective extraction of style features from the image. This capability allows the model to encode the image with CLIP and extract fine-grained style information. Through this alignment, CLIP aids in achieving precise and controllable style transfer in style transfer tasks. However, despite its flexibility, CLIP introduces certain challenges. The core mechanism of CLIP relies on user-defined prompts to fine-tune the style features, which, while offering flexibility, also introduces uncertainty. Specifically, because users can adjust the prompts as needed, there is no strict constraint on the definition of style, which may lead to inconsistencies between the style of the target image and the style description, causing difficulties in maintaining a balance between style and content.

To address this issue, Inversion-Based Style Transfer with Diffusion Models\cite{zhang2023inversion} proposed an innovative style transfer method using CLIP to encode the style image and a U-Net structure in the diffusion model to progressively inject style features into the target image. In this process, the image undergoes a series of noisy and denoising operations, where the style features are gradually transferred to the target image. However, this method is still dependent on the user-provided prompt, and due to the lack of strict constraints on style definition, the consistency between the style image and target image may not always be guaranteed, resulting in unstable or unsatisfactory style transfer outcomes.

To enhance the consistency and precision of style transfer, Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer\cite{yang2023zero} introduced the Zero-Shot Contrastive Loss, which optimizes style transfer through contrastive loss. This approach combines the denoising process in diffusion models with CLIP's style feature extraction, and introduces a multi-region consistency detection strategy to ensure that the style in each region of the image matches the target style. This method significantly improves the precision of style transfer while maintaining the content of the target image, mitigating the style shift problems common in traditional methods.

Further extending this idea, ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and Implicit Style Prompt Bank\cite{Zhang_Zhang_Xing_Li_Zhao_Sun_Lan_Luan_Huang_Lin_2024} introduced a style transfer approach based on an implicit style prompt bank. By combining a pre-trained diffusion model with CLIP's style feature extraction capabilities, this method avoids the dependency on explicit style features. The paper also proposed the Self-Similarity Attention Module (SSAM) to replace the traditional AdaIN module. SSAM ensures precise style transfer through an attention mechanism while preserving the image's structural information, avoiding issues such as style shift or over-smoothing in traditional methods. Another innovation is the introduction of a storage-based SSAM mechanism, which allows the model to flexibly select styles during inference and adjust the style transfer process using a "switch" mechanism. Additionally, the use of random reversal techniques enhances the diversity and robustness of the generated images, further improving the stability of the style transfer process.

Another significant advancement in style transfer using diffusion models is Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer\cite{chung2024style}, which presents a training-free style injection method. This approach blends the target image and style image features by first applying noise to the image through the diffusion model and performing initial style fusion in the latent noise space. Then, through self-attention mechanisms and reverse diffusion, style features are gradually injected into the target image. The innovation lies in its ability to perform style transfer without requiring additional training adjustments, utilizing pre-trained diffusion models instead. This makes the method highly adaptable and versatile across different applications.

In response to the issue of global style imbalance often found in traditional style transfer methods, DiffStyler: Diffusion-based Localized Image Style Transfer\cite{li2024diffstyler} proposed a localized style transfer method. By using low-rank feature adaptation (LOFA) techniques, DiffStyler optimizes the matching of style and content features, improving computational efficiency and ensuring precise style transfer in localized regions while maintaining the overall structure of the image. This localized style transfer method not only improves the precision of style transfer but also enhances computational efficiency, addressing the issue of global style imbalance.

Free Lunch for Text-guided Style Transfer using Diffusion Models\cite{li2024diffstyler} introduced a text-guided style transfer method that combines diffusion models with CLIP, enabling style transfer without additional training. The core concept of this approach is to drive style generation through text prompts, allowing for effective fusion of style and content. The process begins by encoding the text prompts into a style embedding using CLIP, which captures style features such as texture, color palette, and artistic style. The content image is then encoded to extract content features that preserve the image's structure and layout, ensuring the target image retains the original content while undergoing style transformation.The next step involves inputting both content and style features into a U-Net architecture. The U-Net effectively integrates the content and style information, ensuring that the content structure is preserved while the style is accurately injected. The decoder portion of U-Net plays a crucial role in reconstructing the image while adjusting the texture, color, and other artistic features to match the target style. In particular, the high-frequency features (such as details and textures) and low-frequency features (such as structure and background) of the image are implicitly adjusted through latent space representations and denoising processes. This adjustment is not explicitly handled via Fourier Transforms (FFT) like in traditional style transfer methods, but is instead managed through the iterative denoising mechanism of the diffusion model, allowing for fine-tuned adjustments to the imageâ€™s texture, details, and overall style.


In Table \ref{tab:contrast}, we provide a comprehensive evaluation of key works on style transfer, encompassing multiple dimensions such as technical innovation, artistic merit, visual quality, computational efficiency, and creative potential.In summary, several studies, including Inversion-Based Style Transfer with Diffusion Models \cite{zhang2023diffsmooth}, Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer \cite{yang2023zero}, DreamStyler \cite{ahn2023dreamstylerpaintstyleinversion}, CLIPstyler \cite{kwon2022clipstylerimagestyletransfer}, DiffStyler \cite{li2024diffstyler}, and FreeStyle \cite{he2024freestyle}, have leveraged CLIP to adaptively align image styles with textual descriptions, moving beyond the traditional AdaIN-based approach of using fixed means to define style transformations. This paradigm shift allows for more natural and dynamic adaptation to diverse styles, enabling enhanced visual coherence and flexibility in style transfer tasks. However, a persistent challenge remains: the linguistic limitations of CLIP in fully capturing and expressing the nuanced features of artistic styles. Additionally, methods such as ArtBank \cite{Zhang_Zhang_Xing_Li_Zhao_Sun_Lan_Luan_Huang_Lin_2024} and Style Injection in Diffusion \cite{chung2024style} have refined the traditional AdaIN approach, introducing innovations like softmax adjustments or attention mechanisms to improve quality and consistency.

The evident progress in diffusion models since their introduction underscores their immense potential in style transfer applications. Nonetheless, critical challenges lie ahead, particularly in advancing CLIP's ability to "understand" and "interpret" artistic styles, refining style feature extraction for greater precision, and ensuring consistency and stability in single-object style representations. 


\begin{table}[t]

\centering
\tabcolsep=2pt
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccccc}
    \toprule
    {Article} &  {\makecell[c]{Technical\\Innovation}} & {\makecell[c]{Artistic\\ Merit}} & {Visual Quality} &  {\makecell[c]{Computational\\ Efficiency}} & {\makecell[c]{Creative\\ Potential}} & {Year}\\
    \midrule
    \cite{zhang2023inversion} & \usym{2714} & \checkmark  &\checkmark & \usym{2717} & \checkmark & 2023\\
    
    \cite{yang2023zero} & \checkmark & \checkmark  &\checkmark &\usym{2717} & \checkmark & 2023\\
    \cite{Zhang_Zhang_Xing_Li_Zhao_Sun_Lan_Luan_Huang_Lin_2024} & \usym{2714} & \usym{2717}  &\checkmark & \usym{2717} & \usym{2714} & 2023\\
    
    \cite{chung2024style} & \checkmark & \usym{2717}  &\checkmark &\checkmark & \checkmark & 2024 \\
    
    \cite{li2024diffstyler} & \checkmark & \usym{2714}  &\checkmark & \checkmark & \checkmark & 2024\\
    
    \cite{he2024freestyle} & \usym{2714} & \usym{1F5F8}  &\usym{2714}  & \checkmark & \usym{2717} & 2024\\
  
    \bottomrule
\end{tabular}}

\caption{The symbols used in the table follow a standardized rating system: \usym{2714} represents "Excellent", \checkmark indicates "Generally Good", and \usym{2717} denotes "Relatively Low Computational Efficiency". These ratings provide a multifaceted analysis of the papers' performance in the aforementioned areas, offering a quantitative assessment of their contributions and limitations in the field of style transfer research.}
\label{tab:contrast}
\end{table}