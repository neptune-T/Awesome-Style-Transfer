\section{The Era of Generative Adversarial Networks (GAN)}

As deep learning evolved, the need for models that could generate high-quality, realistic images became more prominent. In 2014, Ian Goodfellow et al. introduced a groundbreaking model known as Generative Adversarial Networks (GANs). GANs operate on a unique framework where two neural networks, the \textbf{generator} and the \textbf{discriminator}, engage in a "game" with each other, aiming to improve their respective outputs through competition.

\subsection{ Basic Principles and Architecture of GAN}

Generative Adversarial Networks (GANs), introduced by Ian Goodfellow in 2014, have revolutionized the field of generative modeling by employing a unique adversarial framework. The architecture comprises two neural networks: the generator \(G\) and the discriminator \(D\). The generator transforms random noise \(z\) sampled from a prior distribution \(p_z(z)\) into synthetic data \(G(z)\), while the discriminator \(D\) evaluates whether an input is real (sampled from the true data distribution \(p_{\text{data}}(x)\)) or fake (generated by \(G\)).

The interaction between \(G\) and \(D\) is formulated as a min-max optimization problem:

\begin{align*}
    \min_G \max_D V(D, G) &= \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] \\
    &\quad + \mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))].
\end{align*}


At its theoretical optimum, the discriminator \(D^*\) achieves:

\[
D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)},
\]

where \(p_g(x)\) is the distribution of the generator. Substituting \(D^*\) back into the objective function, the generator’s task reduces to minimizing the Jensen-Shannon (JS) divergence between \(p_{\text{data}}\) and \(p_g\):

\[
C(G) = 2 \cdot \text{JS}(p_{\text{data}} \| p_g) - 2 \log 2.
\]

JS divergence quantifies the similarity between the real and generated distributions, reaching its minimum value when \(p_g = p_{\text{data}}\).GANs have demonstrated remarkable success across various domains, establishing themselves as a cornerstone of modern generative modeling. Their versatility and ability to generate high-quality, realistic outputs have inspired continuous advancements and refinements, solidifying their pivotal role in the field of machine learning.



\subsection{GAN and Its Application in Style Transfe}

However, despite their remarkable potential, the practical training of GANs is often plagued by inherent challenges. The most prominent among these are instability during training and mode collapse. Instability arises when the discriminator overpowers the generator, leading to vanishing gradients that hinder the generator’s ability to learn effectively. Mode collapse, on the other hand, occurs when the generator fails to represent the diversity of the data distribution, repeatedly producing a limited set of outputs. These challenges stem from the adversarial nature of GANs, where balancing the interplay between the generator and discriminator remains a delicate and complex task.

This figure \ref{fig:gan_evolution} outlines the evolution of GANs from their inception, providing a roadmap of key advancements that have significantly influenced the field of generative modeling. Each variant introduced unique contributions to the GAN framework, addressing challenges such as stability, diversity, and application-specific requirements. Following this, we will delve into the innovations and applications of each major GAN variant, emphasizing their role in advancing generative modeling.

To tackle these issues, researchers have proposed a range of improvements and innovative approaches. One groundbreaking refinement was Wasserstein GAN (WGAN)\cite{arjovsky2017wasserstein}, which replaced the Jensen-Shannon divergence with the Wasserstein distance in the loss function. This adjustment not only provided smoother gradients but also improved convergence stability, even when the discriminator is near optimal. Building on this, WGAN-GP introduced gradient penalties to enforce Lipschitz continuity, further stabilizing training and preventing the discriminator from dominating the generator.

Other advancements include spectral normalization, a technique that normalizes the weights of the discriminator to control its capacity and maintain a balanced adversarial dynamic. LSGAN, by employing least-squares loss, mitigated issues such as vanishing gradients, creating a more stable optimization landscape\cite{mao2017least}. Despite these refinements, however, GAN training remains highly sensitive to hyperparameters, necessitating careful tuning to achieve an effective balance between the generator and discriminator.

These foundational developments have laid the groundwork for an array of GAN extensions, each addressing specific shortcomings or exploring novel applications. For instance, CycleGAN introduced cycle-consistency loss, enabling unpaired image-to-image translation by ensuring that mappings between domains are consistent\cite{zhu2017unpaired}. BigGAN pushed the boundaries of high-resolution image generation through large-scale datasets and carefully designed architectures\cite{brock2018large}. Meanwhile, StyleGAN introduced a style-based generator architecture, revolutionizing control over generated images by separating content and style, allowing for intuitive manipulation of image features\cite{karras2019style}.

The iterative progression of these refinements, summarized in \ref{fig:gan_evolution} and depicted in \ref{tab:gan_variants}, highlights the versatility of GANs in adapting to challenges and expanding their scope. Each extension not only advances the field but also sheds light on the limitations of adversarial training, inspiring further research into more stable and efficient generative models.





\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{TPAMI_Survey_Template/figures/table.pdf}
    \caption{Overview of GAN evolution. The diagram showcases the key milestones in the development of GAN variants, highlighting their foundational ideas and unique innovations. It begins with the original GAN (2014)\cite{goodfellow2014generative} and traces subsequent improvements such as DCGAN (2015)\cite{radford2015unsupervised}, CycleGAN (2017)\cite{zhu2017unpaired}, StyleGAN (2019)\cite{karras2019style}, and many others. This figure serves as a foundation for the detailed discussion of these GAN variants in the following sections.}
    \label{fig:gan_evolution}
\end{figure*}






\begin{table*}[h!]
\centering
% \begin{tabular}{c c c c c}
\begin{tabular}{>{\makecell[c]{}}c c c c c}
\hline
\textbf{\makecell[c]{Year}} & \textbf{Method} & \textbf{Author(s)} & \textbf{Publication} & \textbf{Innovation} \\
\hline
2014 & GAN \cite{goodfellow2014generative} & Goodfellow et al. & NeurIPS & Introduced adversarial training for generative models \\
\hline

\multirow{2}{*}{2015}
    & DCGAN \cite{radford2015unsupervised} & Radford et al. & ICLR & Incorporated CNN layers for stable training and improved image quality \\
    & LAPGAN \cite{denton2015lapgan} & Denton et al. & ICML & Laplacian pyramid-based approach for high-resolution image synthesis \\
\hline
\multirow{4}{*}{2016}
    & ImprovedGAN \cite{salimans2016improvedgan} & Salimans et al. & NeurIPS & Techniques for training stability and diversity in GANs \\
    & EBGAN \cite{zhao2016ebgan} & Zhao et al. & arXiv & Energy-based GAN for stability improvement \\
    & VideoGAN \cite{vondrick2016generating} & Vondrick et al. & NeurIPS & GANs for realistic video generation \\
    & InfoGAN \cite{chen2016infogan} & Chen et al. & NeurIPS & Introduced disentanglement by maximizing mutual information \\
\hline
\multirow{10}{*}{2017}
    & CycleGAN \cite{zhu2017unpaired} & Zhu et al. & ICCV & Enabled unpaired image-to-image translation with cycle-consistency loss \\
    & PGGAN \cite{karras2017progressive} & Karras et al. & ICLR & Progressive growing method for high-resolution image synthesis \\
    & WGAN \cite{arjovsky2017wasserstein} & Arjovsky et al. & ICML & Introduced Wasserstein distance to address mode collapse \\
    & LSGAN \cite{mao2017least} & Mao et al. & ICCV & Used least-squares loss for smoother optimization \\
    & BEGAN \cite{berthelot2017began} & Berthelot et al. & ICLR & Balanced generator and discriminator training with autoencoder loss \\
    & Pix2Pix \cite{isola2017pix2pix} & Isola et al. & CVPR & Conditional GAN for paired image-to-image translation \\
    & FID-GAN \cite{heusel2017gans} & Heusel et al. & NeurIPS & Enhanced evaluation metric for GANs based on Fréchet Inception Distance \\
    & DualGAN \cite{yi2017dualgan} & Yi et al. & ICCV & Bidirectional image-to-image translation for paired domains \\
    & SeqGAN \cite{yu2017seqgan} & Yu et al. & AAAI & Sequence generation for text via GAN framework \\
    & SRGAN \cite{ledig2017srgan} & Ledig et al. & CVPR & Super-resolution GAN for high-quality upscaling \\
\hline
\multirow{5}{*}{2018}
    & BigGAN \cite{brock2018large} & Brock et al. & ICLR & Large-scale GAN trained on ImageNet for high-resolution synthesis \\
    & GANimation \cite{pumarola2018ganimation} & Pumarola et al. & ECCV & GAN for facial animation with emotional expressions \\
    & PacGAN \cite{lin2018pacgan} & Lin et al. & NeurIPS & Improved mode collapse with discriminator conditioning \\
    & RecycleGAN \cite{bansal2018recyclegan} & Bansal et al. & CVPR & Addressed temporal consistency in video translation \\
    & StarGAN \cite{choi2018stargan} & Choi et al. & CVPR & Multi-domain image-to-image translation within a single model \\
\hline
\multirow{5}{*}{2019}
     & GauGAN \cite{park2019gaugan} & Park et al. & CVPR & Semantic image synthesis with user-guided input \\
     & StyleGAN \cite{karras2019style} & Karras et al. & CVPR & Style-based generator architecture with control over image features \\
     & GANPaint \cite{bau2019ganpaint} & Bau et al. & SIGGRAPH & Interactive GAN for image editing and content control \\
     & SinGAN \cite{shaham2019singan} & Shaham et al. & ICCV & Single-image GAN for diverse content generation \\
     & SAGAN \cite{zhang2019sagan} & Zhang et al. & ICML & Self-attention GAN for better long-range dependencies in images \\
     
\hline
\multirow{5}{*}{2020}
     & StyleGAN2 \cite{karras2020analyzing} & Karras et al. & CVPR & Improved style control and synthesis quality \\
     & ConSinGAN \cite{hinz2020consingan} & Hinz et al. & CVPR & Single-image GAN for diverse content generation \\
     & PaintGAN \cite{wang2020paintgan} & Wang et al. & ECCV & Brushstroke control for artistic style transfer \\
     & TecoGAN \cite{chu2020tecoGAN} & Chu et al. & ICCV & Temporally consistent GAN for video super-resolution \\
     & U-GAT-IT \cite{kim2020u} & Kim et al. & ICLR & Attention-guided translation with adaptive instance normalization \\
\hline
\multirow{4}{*}{2021}
    & StyleGAN3 \cite{karras2021stylegan3} & Karras et al. & NeurIPS & Mitigated aliasing issues for improved consistency \\
    & GANformer \cite{hudson2021ganformer} & Hudson et al. & CVPR & Hybrid attention GAN combining transformers and GANs \\
    & FastGAN \cite{liu2021fastgan} & Liu et al. & arXiv & Efficient high-resolution image synthesis \\
    & CoModGAN \cite{sauer2021comodgan} & Sauer et al. & CVPR & Conditional modulation for high-resolution inpainting \\

\hline
\multirow{2}{*}{2022}
     & StyleSwin \cite{zhang2022styleswin} & Zhang et al. & CVPR & Transformer-based style synthesis for flexible architecture \\
     & InstGAN \cite{wang2022instgan} & Wang et al. & CVPR & Instance-based GAN for diverse object generation \\
     

\hline
\end{tabular}
\caption{Summary of Key GAN Variants and Their Innovations}
\label{tab:gan_variants}
\end{table*}

