\section{Early Techniques and Traditional Methods}

In the early exploration of style transfer, researchers relied on traditional image processing techniques and handcrafted rules to mimic artistic styles. In the 1990s, Non-Photorealistic Rendering (NPR) became a popular research direction in computer graphics, sparking a range of algorithms aimed at simulating traditional artistic styles. These techniques marked the infancy of style transfer research. Although limited in complexity and adaptability, they provided crucial insights that paved the way for neural style transfer.

\subsection{Image Processing and Handcrafted Style Transfer Techniques}

\textbf{1) Stroke-Based Rendering (SBR): Mimicking Painting Techniques.} In the mid-1990s, Stroke-Based Rendering (SBR) emerged as a pioneering technique for mimicking painting styles. Researchers sought to simulate the look of oil painting or watercolor by layering "strokes" to construct an image with artistic textures. The core idea of SBR is to decompose an image into multiple strokes, each placed according to the shape and edge features of the image, building up layers that give the final image a painterly effect. Mathematically, SBR can be represented as:

\[
I_{\text{rendered}} = \sum_{k=1}^{N} S_k
\]

where \( S_k \) represents each individual stroke. While SBR is capable of producing a single, cohesive style, its reliance on fixed rules limits its flexibility, making it difficult to adapt to diverse styles.

\textbf{2) Region-Based Rendering: Introducing Local Style Control.} As research progressed, scientists explored methods to achieve multiple styles within a single image. Region-based rendering emerged as a technique that segments an image into various regions (such as sky, ground, trees), and applies different styles to each segment. This method brought a breakthrough in local style control for style transfer.

The core of region-based rendering lies in assigning appropriate style weights to each segmented region to achieve localized style control, expressed as:

\[
I(x, y) = \sum_{i=1}^{K} w_i(x, y) \cdot S_i(x, y)
\]

where \( w_i(x, y) \) represents the weight for region \( i \), and \( S_i(x, y) \) represents the style applied to that region. Although this method enhances control over local regions, its effectiveness depends on the accuracy of segmentation and struggles with dynamic style transitions.

\textbf{3) Example-Based Rendering: Introducing Learning Concepts.} Entering the 21st century, researchers began to introduce data-driven approaches to style transfer. In 2001, Hertzmann et al. introduced the "Image Analogies" method, representing this shift\cite{hertzmann2023image}. Example-Based Rendering learns the mapping between a stylized and non-stylized pair of images and applies it to a new input, producing a similar stylized effect. Unlike the fixed-rule methods, Example-Based Rendering leverages examples to capture style characteristics, representing an early attempt to incorporate "learning" into style transfer.

The goal of Example-Based Rendering is to find a mapping \( T \) such that, given a new input image \( I_s' \), it generates a stylized output \( I_t' \) similar to the target style:

\[
I_t' = T(I_s'; I_s, I_t)
\]

where \( T \) is the style mapping function, \( I_s \) and \( I_t \) are paired source and stylized images. While effective for simple style mappings, this method faces challenges in capturing complex, detailed styles.

\textbf{4) Image Filtering and Basic Artistic Effects: Simplified Enhancements of Edges and Blurring.} Around the same time, image filtering techniques became popular for generating simplified artistic effects. Filtering techniques use mathematical operations to adjust the edges and textures of an image, producing specific visual effects. Bilateral filtering and Gaussian blur are two commonly used techniques for creating simple cartoon-like or softened effects.

\begin{itemize}
    \item \textbf{Bilateral Filtering} is an edge-preserving smoothing technique, where the image is smoothed while preserving edge details. 

    \item \textbf{Gaussian Blur} applies a Gaussian kernel to soften the image, generating a dreamy effect. 

    \[
    I_{\text{blurred}}(x, y) = \sum_{x'} \sum_{y'} I(x', y') \cdot G((x, y) - (x', y'))
    \]
\end{itemize}

These filtering methods are computationally efficient and suitable for generating basic artistic effects, but they lack the sophistication required to portray complex, layered styles.

\subsection{The Debut of Deep Learning in Style Transfer}

Under the limitations of traditional image processing technology, it is difficult to simulate diverse artistic styles through style transfer. However, Neural Style Transfer (NST), proposed by Gatys et al. in 2015, provides a groundbreaking solution through deep learning \cite{gatys2015neural}. NST uses Convolutional Neural Networks (CNNs) to automatically extract content and style features, achieving effective decoupling of content and style. This technique leverages the powerful feature extraction capabilities of deep neural networks, enabling style transfer to achieve richer artistic effects and significantly enhancing algorithm flexibility.

The core of NST lies in defining two loss functions: content loss and style loss, which, through an optimization process, blend the structural features of the content image with the texture features of the style image. Content loss preserves the structure of the original content image, while style loss captures the texture and color characteristics of the style image. By minimizing these losses, NST achieves an effective and flexible style transfer, allowing the generation of artistic images that incorporate both the content and the stylized texture.

\textbf{1) Content Loss Function.} The content loss is used to preserve the structural information of the content image, defined as:

   \[
   \mathcal{L}_{\text{content}}(I_C, I_G) = \frac{1}{2} \sum_{i,j} \left( F^l_{I_C}(i,j) - F^l_{I_G}(i,j) \right)^2
   \]

   By minimizing this loss, the generated image progressively retains the structure of the content image, ensuring visual consistency with the content image.

\textbf{2) Style Loss Function.} The style loss captures the texture features of the style image, ensuring that the generated image has a similar visual style to the style image. NST employs a Gram matrix to calculate relationships between image features:

   \[
   G^l_{I}(i,j) = \sum_k F^l_{I}(i,k) F^l_{I}(j,k)
   \]


   where \( G^l_{I}(i,j) \) represents the correlation between channels \( i \) and \( j \) in the \( l \)-th layer feature map. By optimizing the style loss, the generated image can exhibit texture and color characteristics similar to those of the style image.

In the original Neural Style Transfer (NST) method, while the generated results were impressive, the approach relied on an iterative optimization process that was computationally slow, making it challenging to meet real-time application demands. To address this, Johnson et al. proposed a fast style transfer method based on perceptual loss, replacing the iterative optimization process with a pre-trained feed-forward network \cite{johnson2016perceptual}. This approach significantly improved the speed of style transfer while maintaining high image quality, allowing style transfer to be applied in real-time scenarios such as video stylization. Building on this, Ulyanov et al. introduced Texture Networks, another feed-forward style transfer method that bypassed iterative optimization to achieve similar real-time performance \cite{ulyanov2016texture}. Additionally, to further improve the consistency and quality of stylized outputs, especially in preserving fine textures, Ulyanov et al. proposed Instance Normalization, which normalizes each instance independently to eliminate batch dependencies and ensure stable style rendering \cite{ulyanov2016instance}. Together, these innovations made it feasible to use style transfer in low-latency environments, such as mobile devices, while improving both efficiency and image quality.

Although feed-forward networks improved the speed of style transfer, they still required significant computational resources. To address this, Ulyanov et al. proposed the Light Transfer method, which achieves efficient real-time style transfer through a lightweight network design. Compared to other methods, Light Transfer uses fewer parameters, enabling it to run on resource-constrained devices, such as embedded systems and mobile platforms \cite{ulyanov2017light}. This innovation made style transfer technology more accessible in low-resource environments, further promoting its practical applications. Lastly, Risser et al. proposed Histogram Loss to maintain color and texture consistency in style transfer. By using histogram matching, Histogram Loss enhances the tonal and textural details of the generated images, making the stylized image’s color and texture distribution more similar to the style image, thereby improving the visual consistency of the style transfer results \cite{risser2017histogram}. This method adds finer control over color and texture, making it particularly suitable for applications with high artistic quality requirements.

These innovative methods have advanced style transfer technology in terms of computational efficiency, detail control, multi-style adaptation, and resource-efficient usage. With the introduction of these methods, the application scenarios of style transfer technology continue to expand, achieving more expressive and higher-quality image stylization and laying a solid foundation for future research in style transfer technology.

\subsection{Multi-style adaptation and style control}

In the original NST framework, each style transfer was limited to a single style. To meet the need for diverse style transfer, Huang and Belongie proposed the Adaptive Instance Normalization (AdaIN) method, which enables flexible style adaptation by normalizing the mean and variance of feature maps\cite{huang2017arbitrary}. The AdaIN method allows users to customize multiple different styles by adjusting the normalization parameters, thereby enabling arbitrary style transfer. The core formula of this method is:

\[
\text{AdaIN}(x, y) = \sigma(y) \left( \frac{x - \mu(x)}{\sigma(x)} \right) + \mu(y)
\]

where \( x \) represents the content features, \( y \) represents the style features, and \( \mu \) and \( \sigma \) denote the mean and standard deviation of the feature maps, respectively. The introduction of AdaIN brings a high degree of flexibility to style transfer, allowing for multiple styles to be transferred without retraining the model, greatly expanding the applications of NST.

In the initial AdaIN method, the model used fixed normalization parameters for each style, achieving flexible multi-style transfer but with limitations in detailed control. It was challenging to independently adjust styles across different regions of an image. To address this, Zhu et al. proposed Semantic Adaptive Instance Normalization (SEAN), which combines semantic segmentation information to divide the image into multiple semantic regions, assigning unique normalization parameters to each region. This approach enables fine-grained control over local styles, allowing different regions of an image to exhibit distinct style characteristics while preserving the overall style effect \cite{zhu2020sean}.

Building on the success of AdaIN, Huang et al. further improved the method in 2018 by proposing a more flexible approach to adaptive style transfer that retained AdaIN's dynamic style adjustments while enhancing adaptability across a wider range of styles without requiring retraining. This method optimizes AdaIN’s normalization operation by adjusting its parameters to better capture subtle variations in style, allowing the model to adjust to diverse style changes more seamlessly \cite{huang2018multimodal}. This advancement not only increases control over style intensity but also ensures smoother transitions across different styles within a single model, thus expanding AdaIN's applications in complex and dynamic style transfer tasks, such as interactive design and real-time video applications.

To further increase style control within AdaIN-based frameworks, researchers have introduced modifications that allow the user to influence style strength and feature blending in more nuanced ways. These methods enable AdaIN to incorporate style-specific traits while preserving the content’s structural integrity, making it suitable for high-fidelity and context-sensitive style applications. Together, these improvements underscore AdaIN's role as a foundational approach to adaptive style transfer, enhancing the adaptability, control, and applicability of style transfer technology across a range of domains.

Innovations brought by deep learning have expanded the
application scenarios of style transfer from static images
to videos and real-time applications. These breakthroughs
in deep learning also paved the way for the introduction
of generative adversarial networks (GANs) and diffusion
models, allowing style transfer to continue to develop in the
fields of visual arts and creativity.

\begin{table*}[h!]
\centering
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\textbf{Year} & \textbf{\#} & \textbf{Method} & \textbf{Author} & \textbf{Publication} & \textbf{Innovation} & \textbf{Style Transfer Type} \\
\hline
2015 & \cite{gatys2015neural} & NST & Gatys et al. & CVPR & CNN-based style transfer & Single \\
\hline
2016 & \cite{johnson2016perceptual} & Fast Transfer & Johnson et al. & ECCV & Real-time with perceptual loss & Single \\
\hline
2016 & \cite{ulyanov2016texture} & Texture Net & Ulyanov et al. & arXiv & Feed-forward for fast stylization & Single \\
\hline
2016 & \cite{ulyanov2016instance} & Instance Norm & Ulyanov et al. & arXiv & Improved stylization & Single \\
\hline
2017 & \cite{li2017universal} & Universal Style & Li et al. & NIPS & Feature transforms for arbitrary transfer & Arbitrary \\
\hline
2017 & \cite{ulyanov2017improved} & Light Transfer & Ulyanov et al. & CVPR & Lightweight real-time transfer & Single \\
\hline
2017 & \cite{huang2017arbitrary} & AdaIN & Huang et al. & ICCV & Adaptive instance normalization & Arbitrary \\
\hline
2017 & \cite{risser2017histogram} & Histogram Loss & Risser et al. & CVPR & Histogram matching & Single \\
\hline
2018 & \cite{huang2018multimodal} & AdaIN & Huang et al. & CVPR & Adaptive style transfer & Arbitrary \\
\hline
2020 & \cite{zhu2020sean} & SEAN & Zhu et al. & CVPR & Semantic segmentation-based style control & Multi-region \\
\hline
\end{tabular}
\caption{Summary of Key Early Techniques and Traditional Methods in Style Transfer}
\end{table*}

